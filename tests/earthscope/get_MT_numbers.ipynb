{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de3e2f6",
   "metadata": {},
   "source": [
    "# Query EarthScope MT Data Holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fdf9ed-911d-4f0a-839f-8a16de4854ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529c4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kkappler/software/irismt/aurora/tests/earthscope/data_availability/restricted\n"
     ]
    }
   ],
   "source": [
    "doQuery = True\n",
    "doSummary = True\n",
    "doCharts = True\n",
    "include_restricted = True\n",
    "\n",
    "\n",
    "out_folder = pathlib.Path(\".\")\n",
    "out_folder = out_folder.joinpath(\"data_availability\")\n",
    "if include_restricted:\n",
    "    out_folder = out_folder.joinpath(\"restricted\")\n",
    "else:\n",
    "    out_folder = out_folder.joinpath(\"public\") #technically this is public & restricted\n",
    "out_folder.mkdir(exist_ok=True, parents=True)\n",
    "print(out_folder.absolute())\n",
    "\n",
    "filebase = \"mt_availability\"\n",
    "outfile = out_folder.joinpath(f\"{filebase}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c66e90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://service.iris.edu/fdsnws/station/1/query?cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&level=channel&format=text&includecomments=true&nodata=204\n",
      "Identified 20 unique networks: \n",
      " ['1H' '4P' '7I' '8J' '8P' 'AV' 'BK' 'EM' 'II' 'IU' 'N4' 'NV' 'SF' 'SN'\n",
      " 'US' 'XB' 'XC' 'YB' 'Z7' 'ZU']\n",
      "1H\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=1H&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "4P\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=4P&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "7I\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=7I&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "8J\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=8J&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "8P\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=8P&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "AV\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=AV&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "BK\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=BK&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "EM\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=EM&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "II\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=II&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "IU\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=IU&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "N4\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=N4&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "NV\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=NV&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "SF\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=SF&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "SN\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=SN&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "US\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=US&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "XB\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=XB&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "ERROR with availability service http://service.iris.edu/fdsnws/availability/1/query?format=text&net=XB&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204 \n",
      "ERROR: No columns to parse from file\n",
      "XC\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=XC&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "YB\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=YB&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "Z7\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=Z7&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n",
      "ZU\n",
      "http://service.iris.edu/fdsnws/availability/1/query?format=text&net=ZU&cha=?FE,?FN,?FZ,?F1,?F2,?QE,?QN,?QZ,?Q1,?Q2&orderby=nslc_time_quality_samplerate&includerestricted=True&nodata=204\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "fdsn_URL = \"http://service.iris.edu/fdsnws\"\n",
    "channels=['?FE', '?FN', '?FZ','?F1','?F2', '?QE', '?QN', '?QZ', '?Q1', '?Q2']\n",
    "channels = ','.join(channels)\n",
    "\n",
    "if doQuery:\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(\"net.sta,chan,hours\")\n",
    "        \n",
    "    sta_URL=f\"{fdsn_URL}/station/1/query?cha={channels}&level=channel&format=text&includecomments=true&nodata=204\"\n",
    "    print(sta_URL)\n",
    "\n",
    "    try:\n",
    "        network_df=pd.read_csv(sta_URL, sep='|')\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR with station service {sta_URL}\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        quit()\n",
    "\n",
    "    network_df\n",
    "    networks=network_df['#Network '].unique()\n",
    "    print(f\"Identified {len(networks)} unique networks: \\n {networks}\")\n",
    "    # grouped = stations.groupby(by=['#Network ', ' Station '])\n",
    "    # grouped = network_df.groupby(by='#Network ')\n",
    "\n",
    "    for network in networks:\n",
    "        print(network)\n",
    "        netfile = f'{filebase}_{network}.txt'\n",
    "        netfile = out_folder.joinpath(netfile)\n",
    "        \n",
    "        av_URL = f\"{fdsn_URL}/availability/1/query?format=text&net={network}&cha={channels}&orderby=nslc_time_quality_samplerate&includerestricted={include_restricted}&nodata=204\" \n",
    "\n",
    "        print(av_URL)\n",
    "        try:\n",
    "            avail = pd.read_csv(av_URL, sep=\" \")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR with availability service {av_URL} \")\n",
    "            print(f\"ERROR: {e}\")\n",
    "            with open(outfile, 'a') as f:\n",
    "                f.write(f\"\\n#ERROR with {network}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "\n",
    "\n",
    "        avail.columns = avail.columns.str.strip()\n",
    "        avail['Latest'] = pd.to_datetime(avail['Latest'], format=\"%Y-%m-%dT%H:%M:%S.%f\") \n",
    "        avail['Earliest'] = pd.to_datetime(avail['Earliest'],  format=\"%Y-%m-%dT%H:%M:%S.%f\") \n",
    "        avail['Span'] = avail.Latest - avail.Earliest\n",
    "\n",
    "        avail.to_csv(netfile, index=False)\n",
    "        \n",
    "        grouped_chan = avail.groupby(by=['Station','Channel'])\n",
    "        for name, group in grouped_chan:\n",
    "            station = name[0]\n",
    "            channel = name[1]\n",
    "            total_time = group['Span'].sum()\n",
    "            with open(outfile, 'a') as f:\n",
    "                f.write(f\"\\n{network}.{station},{channel},{'%.2f' % (total_time/pd.Timedelta(hours=1))}\")\n",
    "#                 f.write(f\"\\n{network}.{station},{channel},{total_time}\")\n",
    "                \n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5897d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Network</th>\n",
       "      <th>Channels</th>\n",
       "      <th># Stations</th>\n",
       "      <th>Average Time</th>\n",
       "      <th>Total Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1H</td>\n",
       "      <td>[LFE, LFN, LFZ, LQE, LQN]</td>\n",
       "      <td>117</td>\n",
       "      <td>19 days 09:00:00</td>\n",
       "      <td>2267 days 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4P</td>\n",
       "      <td>[LFE, LFN, LFZ, LQE, LQN]</td>\n",
       "      <td>978</td>\n",
       "      <td>22 days 21:00:00</td>\n",
       "      <td>22382 days 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8J</td>\n",
       "      <td>[LFE, LFN, LFZ, LQE, LQN]</td>\n",
       "      <td>25</td>\n",
       "      <td>79 days 04:00:00</td>\n",
       "      <td>1979 days 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8P</td>\n",
       "      <td>[LFE, LFN, LFZ, LQE, LQN]</td>\n",
       "      <td>119</td>\n",
       "      <td>25 days 03:00:00</td>\n",
       "      <td>2989 days 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AV</td>\n",
       "      <td>[LFE, LFN, LFZ]</td>\n",
       "      <td>1</td>\n",
       "      <td>164 days 01:00:00</td>\n",
       "      <td>164 days 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EM</td>\n",
       "      <td>[LFE, LFN, LFZ, LQE, LQN, VFE, VFN, VFZ, VQE, ...</td>\n",
       "      <td>1728</td>\n",
       "      <td>24 days 05:00:00</td>\n",
       "      <td>41841 days 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>II</td>\n",
       "      <td>[LFE, LFN, LFZ, BF1, BF2, BFE, BFN, BFZ, LF1, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4290 days 20:00:00</td>\n",
       "      <td>12872 days 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IU</td>\n",
       "      <td>[LF1, LF2, LFZ, UFZ, VFZ, LFE, LFN]</td>\n",
       "      <td>12</td>\n",
       "      <td>2978 days 10:00:00</td>\n",
       "      <td>41697 days 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>N4</td>\n",
       "      <td>[LF1, LF2, LFZ]</td>\n",
       "      <td>2</td>\n",
       "      <td>975 days 17:00:00</td>\n",
       "      <td>1951 days 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NV</td>\n",
       "      <td>[MF1, MF2, MFZ, MFE, MFN]</td>\n",
       "      <td>2</td>\n",
       "      <td>261 days 01:00:00</td>\n",
       "      <td>522 days 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SN</td>\n",
       "      <td>[CFZ]</td>\n",
       "      <td>4</td>\n",
       "      <td>4 days 22:00:00</td>\n",
       "      <td>19 days 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>US</td>\n",
       "      <td>[LF1, LF2, LFZ]</td>\n",
       "      <td>5</td>\n",
       "      <td>492 days 00:00:00</td>\n",
       "      <td>2460 days 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XC</td>\n",
       "      <td>[MFE, MFN, MFZ, MQE, MQN]</td>\n",
       "      <td>19</td>\n",
       "      <td>19 days 06:00:00</td>\n",
       "      <td>365 days 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>YB</td>\n",
       "      <td>[MFE, MFN, MFZ]</td>\n",
       "      <td>18</td>\n",
       "      <td>863 days 09:00:00</td>\n",
       "      <td>15540 days 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Z7</td>\n",
       "      <td>[MFE, MFN, MFZ, MQE, MQN]</td>\n",
       "      <td>5</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>100 days 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ZU</td>\n",
       "      <td>[LFE, LFN, LFZ, LQE, LQN]</td>\n",
       "      <td>373</td>\n",
       "      <td>25 days 23:00:00</td>\n",
       "      <td>9687 days 02:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Network                                           Channels  # Stations  \\\n",
       "0       1H                          [LFE, LFN, LFZ, LQE, LQN]         117   \n",
       "1       4P                          [LFE, LFN, LFZ, LQE, LQN]         978   \n",
       "2       8J                          [LFE, LFN, LFZ, LQE, LQN]          25   \n",
       "3       8P                          [LFE, LFN, LFZ, LQE, LQN]         119   \n",
       "4       AV                                    [LFE, LFN, LFZ]           1   \n",
       "5       EM  [LFE, LFN, LFZ, LQE, LQN, VFE, VFN, VFZ, VQE, ...        1728   \n",
       "6       II  [LFE, LFN, LFZ, BF1, BF2, BFE, BFN, BFZ, LF1, ...           2   \n",
       "7       IU                [LF1, LF2, LFZ, UFZ, VFZ, LFE, LFN]          12   \n",
       "8       N4                                    [LF1, LF2, LFZ]           2   \n",
       "9       NV                          [MF1, MF2, MFZ, MFE, MFN]           2   \n",
       "10      SN                                              [CFZ]           4   \n",
       "11      US                                    [LF1, LF2, LFZ]           5   \n",
       "12      XC                          [MFE, MFN, MFZ, MQE, MQN]          19   \n",
       "13      YB                                    [MFE, MFN, MFZ]          18   \n",
       "14      Z7                          [MFE, MFN, MFZ, MQE, MQN]           5   \n",
       "15      ZU                          [LFE, LFN, LFZ, LQE, LQN]         373   \n",
       "\n",
       "         Average Time          Total Time  \n",
       "0    19 days 09:00:00  2267 days 01:00:00  \n",
       "1    22 days 21:00:00 22382 days 20:00:00  \n",
       "2    79 days 04:00:00  1979 days 01:00:00  \n",
       "3    25 days 03:00:00  2989 days 07:00:00  \n",
       "4   164 days 01:00:00   164 days 01:00:00  \n",
       "5    24 days 05:00:00 41841 days 02:00:00  \n",
       "6  4290 days 20:00:00 12872 days 12:00:00  \n",
       "7  2978 days 10:00:00 41697 days 19:00:00  \n",
       "8   975 days 17:00:00  1951 days 10:00:00  \n",
       "9   261 days 01:00:00   522 days 03:00:00  \n",
       "10    4 days 22:00:00    19 days 16:00:00  \n",
       "11  492 days 00:00:00  2460 days 02:00:00  \n",
       "12   19 days 06:00:00   365 days 13:00:00  \n",
       "13  863 days 09:00:00 15540 days 11:00:00  \n",
       "14   20 days 00:00:00   100 days 01:00:00  \n",
       "15   25 days 23:00:00  9687 days 02:00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if doSummary:\n",
    "    \n",
    "    summaryDF = pd.DataFrame(columns=['Network','Channels', '# Stations','Average Time', 'Total Time'])    \n",
    "    sumDF = pd.read_csv(outfile)\n",
    "    sumDF[['net','sta']] = sumDF['net.sta'].str.split('.',expand=True)\n",
    "\n",
    "    zDF = sumDF[sumDF.chan.str.endswith('Z', na=False)]\n",
    "\n",
    "\n",
    "    grouped_sum = zDF.groupby(by=['net'])\n",
    "    \n",
    "    for name, group in grouped_sum:\n",
    "        nsta = group.nunique(axis=0)['sta']\n",
    "        avgTime = group['hours'].mean()\n",
    "        totalTime = group['hours'].sum()\n",
    "        chans = sumDF[sumDF['net']==name]['chan'].unique()\n",
    "        newRow = [name, chans, nsta, avgTime, totalTime]\n",
    "        \n",
    "        summaryDF.loc[len(summaryDF.index)] = newRow\n",
    "        \n",
    "\n",
    "           \n",
    "    summaryDF['Average Time'] = summaryDF['Average Time'].round().apply(pd.to_timedelta, unit='H')\n",
    "    summaryDF['Total Time'] = summaryDF['Total Time'].round().apply(pd.to_timedelta, unit='H')\n",
    "    \n",
    "    display(summaryDF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b558cc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doCharts:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m figure, output_file, show, save\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayouts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m column, gridplot\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnDataSource, HoverTool\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "if doCharts:\n",
    "    from bokeh.plotting import figure, output_file, show, save\n",
    "    from bokeh.layouts import column, gridplot\n",
    "    from bokeh.models import ColumnDataSource, HoverTool\n",
    "    from bokeh.models import Range1d\n",
    "    from bokeh.io import output_notebook\n",
    "    \n",
    "    import os\n",
    "    import fnmatch\n",
    "    \n",
    "    \n",
    "    # Get list of all networks that have the availability files\n",
    "    files = fnmatch.filter(os.listdir('.'), f'{filebase}_*.txt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    s = list()\n",
    "    idx = 0\n",
    "    for file in files:\n",
    "        network = file.split('_')[2].split('.')[0]\n",
    "        print(network)\n",
    "        \n",
    "        # Timeline with number of stations depicted by width of line? \n",
    "        # Or two lines that have shaded area between?\n",
    "        thisAvail = pd.read_csv(file,parse_dates=['Earliest','Latest'])\n",
    "        earliest = min(thisAvail['Earliest'])\n",
    "        latest = max(thisAvail['Latest'])\n",
    "        print(earliest, latest)\n",
    "\n",
    "        \n",
    "        # create 100 bins for that timeframe(?)\n",
    "        nbins = 100\n",
    "        datelist = pd.date_range(earliest, latest, periods=100).tolist()\n",
    "\n",
    "        numDF = pd.DataFrame(columns=['Date','Lower','Upper'])    \n",
    "\n",
    "        for date in datelist:\n",
    "            nsta = len(thisAvail[(thisAvail['Earliest']<=date) & (thisAvail['Latest']>=date)].Station.unique())\n",
    "            numDF.loc[len(numDF.index)] = [date, -nsta, nsta]\n",
    "            \n",
    "        \n",
    "        output_notebook()\n",
    "        source = ColumnDataSource(numDF)\n",
    "        \n",
    "        if idx > 0:\n",
    "            tmp_s = figure(width=800, height=100,x_axis_type=\"datetime\",x_range=s[0].x_range)\n",
    "        else: \n",
    "            tmp_s = figure(width=800, height=100,x_axis_type=\"datetime\")\n",
    "        s.append(tmp_s)\n",
    "        \n",
    "        s[idx].varea(x='Date', y1='Upper', y2='Lower', source=source)\n",
    "        s[idx].y_range = Range1d(-30, 30)\n",
    "        s[idx].title = network\n",
    "        s[idx].add_tools(HoverTool(\n",
    "            tooltips=[\n",
    "                ( 'count', '@Upper'),\n",
    "                ( 'time', '@Date'),\n",
    "            ]))\n",
    "        idx+=1\n",
    "    \n",
    "    p = gridplot(s, ncols=1)\n",
    "    show(p)\n",
    "    plot_file_name = out_folder.joinpath(\"mt_numbers.html\")\n",
    "    print(plot_file_name)\n",
    "    output_file(plot_file_name)\n",
    "    save(p)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        # Histogram of length of each station\n",
    "        # Histogram of length of each segment? Do I have that info?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aurora",
   "language": "python",
   "name": "aurora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
