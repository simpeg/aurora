name: Testing

on:
  push:
    branches:
      - '*'
  pull_request:
    branches:
      - '*'
jobs:
  setup-build:
    name: Ex1 (${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        shell: bash
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        # python-version: ["3.10", "3.11", "3.12"]
        python-version: ["3.10"]

    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python ${{ matrix.python-version }}
      run: uv python install ${{ matrix.python-version }}

    - name: Cache MTH5 test files
      uses: actions/cache@v4
      with:
        path: ~/.cache/aurora
        key: mth5-test-files-${{ runner.os }}-${{ hashFiles('tests/conftest.py') }}
        restore-keys: |
          mth5-test-files-${{ runner.os }}-

    - name: Create virtual environment and install dependencies
      run: |
        uv venv --python ${{ matrix.python-version }}
        uv pip install -e ".[dev,test]"
        uv pip install mt_metadata[obspy]
        uv pip install mth5
        uv pip install git+https://github.com/kujaku11/mth5_test_data.git
        uv pip install jupyter ipykernel pytest pytest-cov pytest-timeout codecov

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y pandoc

    - name: Run Tests
      run: |
        source .venv/bin/activate
        pytest -s -v --cov=./ --cov-report=xml --cov=aurora -n auto tests

    - name: Execute Jupyter Notebooks catching failures
      shell: bash
      run: |
        #!/usr/bin/env bash
        set -o pipefail
        # Important: DO NOT use 'set -e' here, because we want to continue on errors.
        
        failures=()
        notebooks=( notebooks/*.ipynb )
        
        for nb in "${notebooks[@]}"; do
          out="executed/$(basename "${nb%.ipynb}").executed.ipynb"
          mkdir -p executed
          echo "Executing: ${nb}"
          jupyter nbconvert \
            --to notebook \
            --execute \
            --ExecutePreprocessor.allow_errors=True \
            --ExecutePreprocessor.timeout=600 \
            --output "${out}" \
            "${nb}"
          rc=$?
          if [[ $rc -ne 0 ]]; then
            echo "⚠️ nbconvert process failed for ${nb} (rc=${rc})."
            failures+=("${nb}")
          fi
        done
        
        # Optional: detect embedded cell errors even when nbconvert returned 0.
        # This lets you continue but fail the job if any notebook captured an error.
        errs=()
        for outnb in executed/*.executed.ipynb; do
          if python - <<'PY'
        import json, sys, glob
        has_err = False
        for fn in glob.glob("executed/*.executed.ipynb"):
            with open(fn, "r", encoding="utf-8") as f:
                nb = json.load(f)
            for cell in nb.get("cells", []):
                if "outputs" in cell:
                    for o in cell["outputs"]:
                        if o.get("output_type") == "error":
                            print(fn)
                            has_err = True
                            raise SystemExit(1)
        raise SystemExit(0)
        PY
          then
            : # no embedded errors found
          else
            errs+=("${outnb}")
          fi
        done
        
        if (( ${#failures[@]} > 0 || ${#errs[@]} > 0 )); then
          echo ""
          echo "======= Summary ======="
          [[ ${#failures[@]} -gt 0 ]] && echo "nbconvert crashed/timeout for:" "${failures[@]}"
          [[ ${#errs[@]} -gt 0 ]]     && echo "Cell errors embedded in:" "${errs[@]}"
          exit 1  # Fail the job overall, but only after running all notebooks
        fi

    # - name: Execute Jupyter Notebooks
    #   run: |
    #     source .venv/bin/activate
    #     python -m ipykernel install --user --name aurora-test
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/examples/dataset_definition.ipynb
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/examples/operate_aurora.ipynb
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/tutorials/pkd_units_check.ipynb
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/tutorials/pole_zero_fitting/lemi_pole_zero_fitting_example.ipynb
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/tutorials/processing_configuration.ipynb
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/tutorials/process_cas04_multiple_station.ipynb
    #     jupyter nbconvert --to notebook --ExecutePreprocessor.allow_errors=True --execute docs/tutorials/synthetic_data_processing.ipynb

    - name: "Upload coverage reports to Codecov"
      uses: codecov/codecov-action@v4
      with:
        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: false
        flags: tests

    - name: Build Doc
      if: ${{ (github.ref == 'refs/heads/main') && (matrix.python-version == '3.8')}}
      run: |
        source .venv/bin/activate
        cd docs
        make html
        cd ..

    - name: GitHub Pages
      if: ${{ (github.ref == 'refs/heads/main') && (matrix.python-version == '3.8')}}
      uses: crazy-max/ghaction-github-pages@v2.5.0
      with:
        build_dir: docs/_build/html
        # Write the given domain name to the CNAME file
        # fqdn: aurora.simpeg.xyz
        # Allow Jekyll to build your site
        jekyll: false # optional, default is true
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
