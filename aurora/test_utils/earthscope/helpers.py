"""
CACHE_PATH: This is a place where all the downloads will land, and summaray csvs will be kept
DATA_AVAILABILITY_PATH: This is a place where information about data availability will be staged
These are txt files generated by Laura's ipynb
DATA_PATH: This is where the mth5 files are archived locally

SPUD_XML_PATH
"""
import datetime
import pandas as pd
import pathlib
import re
import socket
import subprocess

## PLACEHOLDER FOR CONFIG
USE_CHANNEL_WILDCARDS = False
HOSTNAME = socket.gethostname()
HOME = pathlib.Path().home()

if "gadi" in HOSTNAME:
    CACHE_PATH = pathlib.Path("/scratch/my80/kk9397/earthscope")
else:
    CACHE_PATH = HOME.joinpath(".cache").joinpath("earthscope")
CACHE_PATH.mkdir(parents=True, exist_ok=True)
## PLACEHOLDER FOR CONFIG

# Data Availability
DATA_AVAILABILITY_PATH = CACHE_PATH.joinpath("data_availability")
DATA_AVAILABILITY_PATH.mkdir(parents=True, exist_ok=True)
PUBLIC_DATA_AVAILABILITY_PATH = DATA_AVAILABILITY_PATH.joinpath("public")
PUBLIC_DATA_AVAILABILITY_PATH.mkdir(parents=True, exist_ok=True)
RESTRICTED_DATA_AVAILABILITY_PATH = DATA_AVAILABILITY_PATH.joinpath("restricted")
RESTRICTED_DATA_AVAILABILITY_PATH.mkdir(parents=True, exist_ok=True)
DATA_AVAILABILITY_CSV = DATA_AVAILABILITY_PATH.joinpath("MT_acquisitions.csv")

# Data (mth5s)
DATA_PATH = CACHE_PATH.joinpath("data")
DATA_PATH.mkdir(parents=True, exist_ok=True)

# MetaData (mth5s)
EXPERIMENT_PATH = CACHE_PATH.joinpath("experiments")
EXPERIMENT_PATH.mkdir(parents=True, exist_ok=True)

# Transfer Functions
AURORA_TF_PATH = CACHE_PATH.joinpath("aurora_transfer_functions")
AURORA_TF_PATH.mkdir(parents=True, exist_ok=True)

# Summary tables
SUMMARY_TABLES_PATH = CACHE_PATH.joinpath("summary_tables")
SUMMARY_TABLES_PATH.mkdir(parents=True, exist_ok=True)

SPUD_XML_PATHS = {}
SPUD_XML_PATHS["base"] = CACHE_PATH.joinpath("spud_xml")
SPUD_XML_PATHS["base"].mkdir(parents=True, exist_ok=True)
SPUD_XML_PATHS["data"] = SPUD_XML_PATHS["base"].joinpath("data")
SPUD_XML_PATHS["data"].mkdir(parents=True, exist_ok=True)
SPUD_XML_PATHS["emtf"] = SPUD_XML_PATHS["base"].joinpath("emtf")
SPUD_XML_PATHS["emtf"].mkdir(parents=True, exist_ok=True)

def strip_xml_tags(some_string):
    """
    Allows simplification of less intuitive (albeit faster) commands such as:
    cmd = f"grep 'SourceData id' {emtf_filepath} | awk -F'"'"'"' '{print $2}'"
    qq = subprocess.check_output([cmd], shell=True)
    data_id = int(qq.decode().strip())
    with
    cmd = f"grep 'SourceData id' {emtf_filepath}"
    qq = subprocess.check_output([cmd], shell=True)
    qq = strip_xml_tags(qq)
    data_id = int(qq.decode().strip())
    :param some_string:
    :return:
    """
    stripped = re.sub('<[^>]*>', '', some_string)
    return stripped

def get_via_curl(source, target):
	"""
	If exit_status of 127 is returned you may need to install curl in your environment
	If you need a file with the IRIS mda string, i_row=6000 has one.

	Note that the EMTF spuds come as HTML, to get XML need to edit the curl command, adding
	-H 'Accept: application/xml'
	https://stackoverflow.com/questions/22924993/getting-webpage-data-in-xml-format-using-curl

	ToDo: confirm the -H option works OK for DATA_URL as well.

	Parameters
	----------
	source
	target

	Returns
	-------

	"""
	cmd = f"curl -s -H 'Accept: application/xml' {source} -o {target}"
	print(cmd)
	exit_status = subprocess.call([cmd], shell=True)
	if exit_status != 0:
		print(f"Failed to {cmd}")
		raise Exception
	return

def load_xml_tf(file_path):
    """
    using emtf_xml path will fail with KeyError: 'field_notes'
    :param file_path:
    :return:
    """
    from mt_metadata.transfer_functions.core import TF
    # if "15029445_EM_PAM57" in str(file_path):
    #     print("debug")
    print(f"reading {file_path}")
    spud_tf = TF(file_path)
    spud_tf.read()
    return spud_tf


def get_remotes_from_tf_not(tf_obj):
    remote_references = tf_obj.station_metadata.get_attr_from_name('transfer_function.remote_references')
    remotes = list()
    for remote_station in remote_references:
        if not len(remote_station.split('-')) > 1:
            if remote_station != station:
                remotes.append(remote_station)

    return remotes

def get_rr_info(tf_obj):
    rr_info_list = tf_obj.station_metadata.transfer_function.processing_parameters
    # this may have more than one entry .. why?
    assert len(rr_info_list) == 1
    rr_info_instance = rr_info_list[0]
    return rr_info_instance

def get_rr_type(tf_obj):
    rr_info_instance = get_rr_info(tf_obj)
    rr_type = rr_info_instance["remote_ref.type"]
    return rr_type


def get_remotes_from_tf_2(tf_obj):
    """
    A second way to get remotes
    :param tf_obj:
    :return:
    """
    attr_name = "transfer_function.remote_references"
    remote_references = tf_obj.station_metadata.get_attr_from_name(attr_name)
    remotes = list()
    for remote_station in remote_references:
        if not len(remote_station.split('-')) > 1:
            # if remote_station != station:
            remotes.append(remote_station)
    print(remote_references)
    return remotes

def get_remotes_from_tf(tf_obj):
    """
    There were 5 cases of RemoteRef type encountered when reviewing SPUD TFs
    These were:
    1. Robust Remote Reference           1452
    2. Robust Multi-Station Reference     328
    3. Robust Multi-station Reference      15
    4. Multi-Station Reference              1
    5. Merged Transfer Functions           52
    where the number of instaces of each is listed on the right.


    :param tf_obj:
    :return:
    """
    rr_info_instance = get_rr_info(tf_obj)
    if rr_info_instance["remote_ref.type"] == "Robust Remote Reference":
        # then there is only one
        try:
            remotes = [rr_info_instance["remote_info.site.id"], ]
        except KeyError:
            print(" No remote listed in xml at expected location")
            # here an an example: https: // ds.iris.edu / spudservice / data / 14862696
            return []
    else:
        remotes = get_remotes_from_tf_2(tf_obj)
    return remotes

def build_request_df(station_id, network_id, channels=None, start=None, end=None):
    if channels is None:
        channels = "*"
        print("this doesn't work")

    # need this for columns
    from mth5.clients import FDSN
    fdsn_object = FDSN(mth5_version='0.2.0')
    fdsn_object.client = "IRIS"
    if start is None:
        start = '1970-01-01 00:00:00'
    if end is None:
        end = datetime.datetime.now()
        end = end.replace(hour=0, minute=0, second=0, microsecond=0)
        end = str(end)

    request_list = []
    for channel in channels:
        request_list.append([network_id, station_id, '', channel, start, end])

    print(request_list)

    request_df = pd.DataFrame(request_list, columns=fdsn_object.request_columns)
    return request_df


def get_summary_table_filename(stage_number):
    base_names = {}
    base_names["00"] = "spud_xml_scrape"
    base_names["01"] = "spud_xml_review"
    base_names["02"] = "local_metadata_coverage"
    base_names["03"] = "local_mth5_coverage"
    base_names["04"] = "processing_review"
    base_names["05"] = "tf_comparison_review"
    stage_number_str = str(stage_number).zfill(2)

    now = datetime.datetime.now().__str__().split(".")[0].replace(" ", "_")
    now_str = now.replace(":", "")
    csv_name = f"{stage_number_str}_{base_names[stage_number_str]}.csv"
    if stage_number in [1,]:
        now = datetime.datetime.now().__str__().split(".")[0].replace(" ", "_")
        now_str = now.replace(":", "")
        csv_name = csv_name.replace(".csv", f"_{now_str}.csv")

    csv_path = SUMMARY_TABLES_PATH.joinpath(csv_name)

    return csv_path


def get_most_recent_summary_filepath(stage_number):
    """
    For each stage of task 1, there is a summary table produced, and that summary table is used
    as input for the next stage of the process.  These tables are timestamped.
    Normally we want the most recent one, and we don't want to be pasting filenames all over the place
    This returns the path to the most recent table.

    :param stage_number:
    :return:
    """
    stage_number_str = str(stage_number).zfill(2)
    globby = SUMMARY_TABLES_PATH.glob(f"{stage_number_str}*")
    globby = list(globby)
    globby.sort()
    return globby[-1]

def load_most_recent_summary(stage_number):
    review_csv = get_most_recent_summary_filepath(stage_number)
    print(f"loading {review_csv}")
    results_df = pd.read_csv(review_csv)
    return results_df


def load_data_availability_dfs():
    output = {}
    globby = PUBLIC_DATA_AVAILABILITY_PATH.glob("*txt")
    for txt_file in globby:
        print(txt_file)
        network_id = txt_file.name.split("_")[-1].split(".txt")[0]
        df = pd.read_csv(txt_file, parse_dates=['Earliest', 'Latest', ])
        output[network_id] = df
        print(f"loaded {network_id}")
    return output

class DataAvailability(object):
    def __init__(self):
        self.df_dict = load_data_availability_dfs()

    def get_available_channels(self, network_id, station_id):
        availability_df = self.df_dict[network_id]
        sub_availability_df = availability_df[availability_df["Station"] == station_id]
        availabile_channels = sub_availability_df['Channel'].unique()
        return availabile_channels


KEEP_COLUMNS = ['emtf_id', 'data_id','file_size','data_xml_filebase',
                'data_error', 'data_remote_ref_type', 'data_remotes',]
def restrict_to_mda(df, RR=None, keep_columns=KEEP_COLUMNS):
    """
    Takes as input the summary from xml ingest (process 01) and restricts to rows where
    data at IRIS/Earthscope are expected.
    :param df:
    :param RR:
    :param keep_columns:
    :return:
    """
    n_xml = len(df)
    is_not_mda = df.data_xml_filebase.str.contains("__")
    n_non_mda = is_not_mda.sum()
    n_mda = len(df) - n_non_mda
    print(f"There are {n_mda} / {n_xml} files with mda string ")
    print(f"There are {n_non_mda} / {n_xml} files without mda string ")
    mda_df = df[~is_not_mda]
    mda_df.reset_index(drop=True, inplace=True)


    if RR:
        is_rrr = mda_df.data_xml_path_remote_ref_type == RR
        mda_df = mda_df[is_rrr]
        mda_df.reset_index(drop=True, inplace=True)

    mda_df = mda_df[keep_columns]

    fix_nans_in_columns = ["data_remotes",]
    for col in fix_nans_in_columns:
        if col in mda_df.columns:
            mda_df[col] = mda_df[col].astype(str)
            mda_df[mda_df[col]=="nan"][col] = ""

    print("ADD NETWORK/STATION COLUMNS for convenience")
    print("Consdier PUSH THIS BACK TO TASK 01 once all XML are reading successfully")
    # Get station/Networks
    xml_source = 'data'
    n_rows = len(mda_df)
    networks = n_rows * [""]
    stations = n_rows * [""]
    for i, row in mda_df.iterrows():
        #xml_path = SPUD_XML_PATHS[xml_source].joinpath(row[f"{xml_source}_xml_filebase"])
        xml_filebase = row[f"{xml_source}_xml_filebase"]
        xml_filestem = xml_filebase.split(".")[0]
        [xml_uid, network_id, station_id] = xml_filestem.split("_")
        networks[i] = network_id
        stations[i] = station_id
    mda_df["station_id"] = stations
    mda_df["network_id"] = networks

    return mda_df
