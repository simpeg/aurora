"""
This may wind up in aurora/transfer_function/tf_kernel/dataset.py

2022-06-21: Actually after more consideration, this class should be
called MTH5Dataset(), and should live in mth5_helpers.

It should leverage the @staticmethod decorator so that it returns a modified df
frame.
class MWD():
    def __init__(self):
        pass

    @staticmethod

That
"""

import copy
import pandas as pd

import mth5
from mth5.utils.helpers import initialize_mth5


INPUT_CHANNELS = ["hx", "hy", ]
OUTPUT_CHANNELS = ["ex", "ey", "hz", ]
RUN_SUMMARY_COLUMNS = ["station_id", "run_id", "start", "end", "sample_rate",
                       "input_channels", "output_channels", "remote", "mth5_path"]



class RunSummary():
    """
    Could be called "ProcessableDataset", KernelDataset, InputDataset or something
    like that.  This class is intended to work with mth5-derived channel_summary or
    run_summary dataframes, that specify time series intervals. that is

    The main idea is to specify one or two stations, together
    with a list of acquisition "runs" that can be merged into a "processing run".
    Each acquistion run can be further divided into non-overlapping chunks by specifying
    time-intervals associated with that acquistion run.  An empty iterable of
    time-intervals associated with a run is interpretted as the interval
    corresponding to the entire run.

    The time intervals can be used for several purposes but primarily:
    To specify contiguous chunks of data:
    1.  to STFT, that will be made into merged FC data structures
    2. to bind together into xarray time series, for eventual gap fill (and then STFT)
    3. To manage and analyse the availability of reference time series

    The basic data strucutre can be represented as a table or as a tree:
    Station --> run --> [Intervals],
    where the --> symbol is reads "branches that specify (a)".

    This is described in issue #118 https://github.com/simpeg/aurora/issues/118

    Desired Properties
    a) This should be able to take a dictionary (tree) and return the tabular (
    DataFrame) representation and vice versa.
    b) Ability (when there are two or more stations) apply interval intersection
    rules, so that only time intervals when both stations are acquiring data are
    returned

    From (a) above we can see that a simple table per station can
    represent the available data.  That table can be generated by default from
    the mth5, and intervals to exclude some data can be added as needed.

    (b) is really just the case of considering pairs of tables like (a)

    Thinking all that through, we actually want a baseclass StationDataset.
    The RR case could be handled by pairing two of these; StationPairDataset.  That
    would make this thing here a MultiStationDataset.

    The dependencies aren't clear yet.
    Maybe still Dataset:
        Could have methods
            "drop_runs_shorter_than"
            "fill_gaps_by_time_interval"
            "fill_gaps_by_run_names"
            "
    ?MultiStation isa Station
    ?StationPair isa Station


    In a perfect world, we would write the ChannelDataset class here, and make
    StationDataset a collection of ChannelDatasets, but that _should_ be add-inable
    later.  For the full MMT case we could then consider ChannelPairDataset objects.


    2022-03-11:
    Following notes in Issue #118, want to get a fully populated dataframe from an mth5.
    If I pass a station_id, then get all runs, if I pass a (station_id, run_id),
    then just get the run start and end times.

    Question: To return a copy or modify in-place when querying.  Need to decide on
    standards and syntax.  Handling this in general is messy because every function
    needs to be modified.  Maybe better to use a decorator that allows for df kwarg
    to be passed, and if it is not passed the modification is done in place.
    The user who doesn't want to modify in place can work with a clone.

    """
    def __init__(self, **kwargs):
        self.columns = ["station_id", "run_id", "start", "end"]
        self.column_dtypes = [str, str, pd.Timestamp, pd.Timestamp]
        self._input_dict = kwargs.get("input_dict", None)
        self.df = kwargs.get("df", None)


    def clone(self):
        return copy.deepcopy(self)


    def from_mth5s(self, mth5_list):
        run_summary_df = extract_run_summaries_from_mth5s(mth5_list)
        self.df = run_summary_df



    def add_duration(self, df=None):
        """

        Parameters
        ----------
        df

        Returns
        -------

        """
        if df is None:
            df = self.df
        timedeltas = df.end - df.start
        durations = [x.seconds for x in timedeltas]
        df["duration"] = durations
        return

    def drop_runs_shorter_than(self, duration, units="s"):
        if units != "s":
            raise NotImplementedError
        if "duration" not in self.df.columns:
            self.add_duration()
        drop_cond = self.df.duration < duration
        #df = self.df[drop_cond]
        self.df.drop(self.df[drop_cond].index, inplace=True)
        df = df.reset_index()


        self.df = df
        return df

def channel_summary_to_run_summary(ch_summary,
                                   allowed_input_channels=INPUT_CHANNELS,
                                   allowed_output_channels=OUTPUT_CHANNELS,
                                   sortby=["station_id", "run_id", "start"]):
    """
    TODO: replace station_id with station, and run_id with run
    Note will need to modify: aurora/tests/config$ more test_dataset_dataframe.py
    TODO: Add logic for handling input and output channels based on channel
    summary.  Specifically, consider the case where there is no vertical magnetic
    field, this information is available via ch_summary, and output channels should
    then not include hz.
    TODO: Just inherit all the run-level and higher el'ts of the channel_summary,
    including n_samples?

    When creating the dataset dataframe, make it have these columns:
    [
            "station_id",
            "run_id",
            "start",
            "end",
            "mth5_path",
            "sample_rate",
            "input_channels",
            "output_channels",
            "remote",
	    "channel_scale_factors",
        ]

    Parameters
    ----------
    ch_summary: mth5.tables.channel_table.ChannelSummaryTable or pandas DataFrame
       If its a dataframe it is a representation of an mth5 channel_summary.
        Maybe restricted to only have certain stations and runs before being passed to
        this method
    allowed_input_channels: list of strings
        Normally ["hx", "hy", ]
        These are the allowable input channel names for the processing.  See further
        note under allowed_output_channels.
    allowed_output_channels: list of strings
        Normally ["ex", "ey", "hz", ]
        These are the allowable output channel names for the processing.
        A global list of these is kept at the top of this module.  The purpose of
        this is to distinguish between runs that have different layouts, for example
        some runs will have hz and some will not, and we cannot process for hz the
        runs that do not have it.  By making this a kwarg we sort of prop the door
        open for more general names (see issue #74).
    sortby: bool or list
        Default: ["station_id", "run_id", "start"]

    Returns
    -------
    run_summary_df: pd.Dataframe
        A table with one row per "acquistion run" that was in the input channel
        summary table
    """
    if isinstance(ch_summary, mth5.tables.channel_table.ChannelSummaryTable):
        ch_summary_df = ch_summary.to_dataframe()
    elif isinstance(ch_summary, pd.DataFrame):
        ch_summary_df = ch_summary
    grouper = ch_summary_df.groupby(["station", "run"])
    n_station_runs = len(grouper)
    station_ids = n_station_runs * [None]
    run_ids = n_station_runs * [None]
    start_times = n_station_runs * [None]
    end_times = n_station_runs * [None]
    sample_rates = n_station_runs * [None]
    input_channels = n_station_runs * [None]
    output_channels = n_station_runs * [None]
    channel_scale_factors = n_station_runs * [None]
    i = 0
    for (station_id, run_id), group in grouper:
        #print(f"{i} {station_id} {run_id}")
        #print(group)
        station_ids[i] = station_id
        run_ids[i] = run_id
        start_times[i] = group.start.iloc[0]
        end_times[i] = group.end.iloc[0]
        sample_rates[i] = group.sample_rate.iloc[0]
        channels_list = group.component.to_list()
        num_channels = len(channels_list)
        input_channels[i] = [x for x in channels_list if x in allowed_input_channels]
        output_channels[i] = [x for x in channels_list if x in allowed_output_channels]
        channel_scale_factors[i] = dict(zip(channels_list, num_channels*[1.0]))
        i += 1

    data_dict = {}
    data_dict["station_id"] = station_ids
    data_dict["run_id"] = run_ids
    data_dict["start"] = start_times
    data_dict["end"] = end_times
    data_dict["sample_rate"] = sample_rates
    data_dict["input_channels"] = input_channels
    data_dict["output_channels"] = output_channels
    data_dict["channel_scale_factors"] = channel_scale_factors

    run_summary_df = pd.DataFrame(data=data_dict)
    if sortby:
        run_summary_df.sort_values(by=sortby, inplace=True)
    return run_summary_df



def extract_run_summary_from_mth5(mth5_obj, summary_type="run"):
    """

    Parameters
    ----------
    mth5_obj: mth5.mth5.MTH5
        The initialized mth5 object that will be interrogated
    summary_type: str
        One of ["run", "channel"].  Returns a run summary or a channel summary

    Returns
    -------
    out_df: pd.Dataframe
        Table summarizing the available runs in the input mth5_obj
    """
    channel_summary_df = mth5_obj.channel_summary.to_dataframe()
    #check that the mth5 has been summarized already
    if len(channel_summary_df) < 2:
        print("The channel summary may not have been initialized yet, at least 4 "
              "channels are expected.")
        mth5_obj.channel_summary.summarize()
        channel_summary_df = mth5_obj.channel_summary.to_dataframe()
    if summary_type=="run":
        out_df = channel_summary_to_run_summary(channel_summary_df)
    else:
        out_df = channel_summary_df
    out_df["mth5_path"] = str(mth5_obj.filename)
    return out_df


def extract_run_summaries_from_mth5s(mth5_list, summary_type="run", deduplicate=True):
    """
    ToDo: Move this method into mth5? or mth5_helpers?
    ToDo: Make this a class so that the __repr__ is a nice visual representation of the
    df, like what channel summary does in mth5

    2022-05-28 Modified to allow this method to accept mth5 objects as well as the
    already supported types of pathlib.Path or str

    Given a list of mth5s, this returns a dataframe of all available runs

    In order to drop duplicates I used the solution here:
    https://stackoverflow.com/questions/43855462/pandas-drop-duplicates-method-not-working-on-dataframe-containing-lists

    Parameters
    ----------
    mth5_paths: list
        paths or strings that point to mth5s
    summary_type: string
        one of ["channel", "run"]
        "channel" returns concatenated channel summary,
        "run" returns concatenated run summary,
    deduplicate: bool
        Default is True, deduplicates the summary_df

    Returns
    -------
    super_summary: pd.DataFrame

    """
    dfs = len(mth5_list) * [None]

    for i, mth5_elt in enumerate(mth5_list):
        if isinstance(mth5_elt, mth5.mth5.MTH5):
            mth5_obj = mth5_elt
        else:   #mth5_elt is a path or a string
            mth5_obj = initialize_mth5(mth5_elt, mode="a")

        df = extract_run_summary_from_mth5(mth5_obj, summary_type=summary_type)

        #close it back up if you opened it
        if not isinstance(mth5_elt, mth5.mth5.MTH5):
            mth5_obj.close_mth5()
        dfs[i] = df

    #merge all summaries into a super_summary
    super_summary = pd.concat(dfs)
    super_summary.reset_index(drop=True, inplace=True)
    if deduplicate:
        keep_indices = super_summary.astype(str).drop_duplicates().index
        super_summary = super_summary.loc[keep_indices]
        super_summary.reset_index(drop=True, inplace=True)
    return super_summary