"""
Helper class to make config files.

Note: the config is still evolving and this class and its methods are expected to
change.


"""
from deprecated import deprecated
from pathlib import Path

from aurora.config import Processing, Station, Run, BANDS_DEFAULT_FILE


class ConfigCreator:
    def __init__(self, **kwargs):
        default_config_path = Path("config")
        self.config_path = kwargs.get("config_path", default_config_path)

    def processing_id(self, kernel_dataset):
        """
        In the past, we used f"{local}-{remote}" or  f"{local}-{run_id}"
        Neither of these is sufficiently unique.  In fact, they only describe the
        dataset, and not the processing config.  It is difficult to see how to make a
        comprehensive, unique id without it being very long or involving hash
        functions.

        For now, will try to use {local}-{local_runs}-{remote}-{remote_runs},
        which at least describes the dataset, then a string can be generated by the
        config and appended if needed.


        Parameters
        ----------
        kernel_dataset

        Returns
        -------

        """
        id = f"{kernel_dataset.local_station_id}-{kernel_dataset.remote_station_id}"
        return id

    def create_from_kernel_dataset(
        self,
        kernel_dataset,
        emtf_band_file=BANDS_DEFAULT_FILE,
        input_channels=["hx", "hy"],
        output_channels=["hz", "ex", "ey"],
        estimator=None,
        band_edges=None,
        **kwargs,
    ):
        """
        Hmmm, why not make this a method of kernel_dataset??

        Parameters
        ----------
        kernel_dataset
        emtf_band_file
        input_channels
        output_channels
        estimator
        band_edges
        kwargs

        Returns
        -------

        """

        processing_id = self.processing_id(kernel_dataset)
        processing_obj = Processing(id=processing_id, **kwargs)

        # pack station and run info into processing object
        processing_obj.stations.from_dataset_dataframe(kernel_dataset.df)

        processing_obj.set_frequency_bands(
            emtf_band_file=emtf_band_file,
            band_edges=band_edges,
            sample_rate=kernel_dataset.sample_rate,
        )
        for key, decimation_obj in processing_obj.decimations_dict.items():
            decimation_obj.input_channels = input_channels
            decimation_obj.output_channels = output_channels
            # set estimator if provided as kwarg
            if estimator:
                try:
                    decimation_obj.estimator.engine = estimator["engine"]
                except KeyError:
                    pass
        return processing_obj

    # @deprecated
    # def create_run_processing_object(
    #     self,
    #     station_id=None,
    #     run_id=None,
    #     mth5_path=None,
    #     sample_rate=1,
    #     input_channels=["hx", "hy"],
    #     output_channels=["hz", "ex", "ey"],
    #     estimator=None,
    #     emtf_band_file=BANDS_DEFAULT_FILE,
    #     band_edges=None,
    #     **kwargs,
    # ):
    #     """
    #     Create a default processing object
    #
    #     ToDo: Consider deprecating this method in lieu of create_from_kernel_dataset
    #
    #     Parameters
    #     ----------
    #     station_id
    #     run_id: string or list of strings.
    #     mth5_path
    #     sample_rate
    #     input_channels
    #     output_channels
    #     estimator
    #     emtf_band_file
    #     kwargs
    #
    #     Returns
    #     -------
    #     processing_obj: aurora.config.metadata.processing.Processing
    #     """
    #     processing_id = f"{station_id}-{run_id}"
    #     processing_obj = Processing(id=processing_id, **kwargs)
    #
    #     if not isinstance(run_id, list):
    #         run_id = [run_id]
    #
    #     runs = []
    #     for run in run_id:
    #         run_obj = Run(
    #             id=run_id,
    #             input_channels=input_channels,
    #             output_channels=output_channels,
    #             sample_rate=sample_rate,
    #         )
    #         runs.append(run_obj)
    #
    #     station_obj = Station(id=station_id, mth5_path=mth5_path)
    #     station_obj.runs = runs
    #
    #     processing_obj.stations.local = station_obj
    #     processing_obj.set_frequency_bands(
    #         emtf_band_file=emtf_band_file,
    #         band_edges=band_edges,
    #         sample_rate=sample_rate,
    #     )
    #     for key in sorted(processing_obj.decimations_dict.keys()):
    #         decimation_obj = processing_obj.decimations_dict[key]
    #         decimation_obj.input_channels = input_channels
    #         decimation_obj.output_channels = output_channels
    #         # set estimator if provided as kwarg
    #         if estimator:
    #             try:
    #                 decimation_obj.estimator.engine = estimator["engine"]
    #             except KeyError:
    #                 pass
    #     return processing_obj

    def to_json(self, processing_obj, path=None, nested=True, required=False):
        """
        Write a processing object to path

        :param path: DESCRIPTION
        :type path: TYPE
        :param processing_obj: DESCRIPTION
        :type processing_obj: TYPE
        :return: DESCRIPTION
        :rtype: TYPE

        """
        if path is None:
            json_fn = processing_obj.json_fn()  # config_id + "_run_config.json"
            self.config_path.mkdir(exist_ok=True)
            path = self.config_path.joinpath(json_fn)
        with open(path, "w") as fid:
            json_obj = processing_obj.to_json(nested=nested, required=required)
            fid.write(json_obj)
